{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja 8 minería de datos\n",
    "## Pasos 1 a 2\n",
    "\n",
    "### PASO 2\n",
    "sale price será variable de respuesta, mendiante cuartiles se divide en barato, medio y cara. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      OverallQual  YearBuilt  YearRemodAdd  TotalBsmtSF  1stFlrSF  GrLivArea  \\\n",
      "0               7       2003          2003          856       856       1710   \n",
      "1               6       1976          1976         1262      1262       1262   \n",
      "2               7       2001          2002          920       920       1786   \n",
      "3               7       1915          1970          756       961       1717   \n",
      "4               8       2000          2000         1145      1145       2198   \n",
      "...           ...        ...           ...          ...       ...        ...   \n",
      "1455            6       1999          2000          953       953       1647   \n",
      "1456            6       1978          1988         1542      2073       2073   \n",
      "1457            7       1941          2006         1152      1188       2340   \n",
      "1458            5       1950          1996         1078      1078       1078   \n",
      "1459            5       1965          1965         1256      1256       1256   \n",
      "\n",
      "      FullBath  TotRmsAbvGrd  GarageCars  GarageArea  SalePrice PriceCategory  \n",
      "0            2             8           2         548     208500             1  \n",
      "1            2             6           2         460     181500             1  \n",
      "2            2             6           2         608     223500             2  \n",
      "3            1             7           3         642     140000             1  \n",
      "4            2             9           3         836     250000             2  \n",
      "...        ...           ...         ...         ...        ...           ...  \n",
      "1455         2             7           2         460     175000             1  \n",
      "1456         2             7           2         500     210000             1  \n",
      "1457         2             9           1         252     266500             2  \n",
      "1458         1             5           1         240     142125             1  \n",
      "1459         1             6           1         276     147500             1  \n",
      "\n",
      "[1460 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dataset de hojas anteriores ( inciso 1)\n",
    "\n",
    "df  = pd.read_csv(\"train.csv\")\n",
    "numeric_columns = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n",
    "                   'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "                   'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n",
    "                   'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
    "                   'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n",
    "                   'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
    "                   'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold','SalePrice']\n",
    "numeric_df = df[numeric_columns]\n",
    "corr_matrix = numeric_df.corr()\n",
    "saleprice_correlation = corr_matrix['SalePrice'].sort_values(ascending=False)\n",
    "# Filtrar las variables numéricas con correlación menor a 0.5 con SalePrice\n",
    "to_drop = saleprice_correlation[saleprice_correlation < 0.5].index.tolist()\n",
    "cleaned_df = numeric_df.drop(columns=to_drop)\n",
    "\n",
    "p25 = cleaned_df['SalePrice'].quantile(0.25)\n",
    "p75 = cleaned_df['SalePrice'].quantile(0.75)\n",
    "cleaned_df['PriceCategory'] = pd.cut(cleaned_df['SalePrice'], bins=[0, p25, p75, float('inf')],\n",
    "                               labels=[0,1,2], right=False)\n",
    "df = cleaned_df\n",
    "del cleaned_df,numeric_columns,to_drop,corr_matrix,saleprice_correlation,p25,p75\n",
    "print(df)\n",
    "# variable de respuesta ['SalePrice'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Rregression AI\n",
    "y_R = df['SalePrice']\n",
    "X_R = df.drop('SalePrice',axis=1).drop('PriceCategory',axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R, y_R, test_size=0.2, random_state=42)\n",
    "\n",
    "#Same neurons as variables (uses lineal regression)\n",
    "regressorVars = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
    "#Aristocratic with mini batches (uses lineal regression)\n",
    "regressorAristo = MLPRegressor(random_state=1, max_iter=500,batch_size=min(200, X_R.shape[1])).fit(X_train, y_train)\n",
    "#hyperbolic regression model \n",
    "regressorHyper = MLPRegressor(random_state=1,max_iter=500,activation='tanh').fit(X_train, y_train)\n",
    "\n",
    "#Classification AI\n",
    "y_C = df['PriceCategory']\n",
    "X_C = df.drop('SalePrice',axis=1).drop('PriceCategory',axis=1)\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_C, y_C, test_size=0.2, random_state=42)\n",
    "\n",
    "#default classifier\n",
    "stochasticClassifier = MLPClassifier(random_state=1, max_iter=300, solver='adam').fit(X_train_c, y_train_c) #stochastic gradient-based optimizer \n",
    "gradienntClassifier = MLPClassifier(random_state=1, max_iter=300, solver='sgd').fit(X_train_c, y_train_c) #gradient stochasting descent\n",
    "quasiClassifier = MLPClassifier(random_state=1, max_iter=300, solver='lbfgs').fit(X_train_c, y_train_c) #quasi-Newton methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use los modelos para predecir el valor de la variable respuesta\n",
    "## 5. Haga las matrices de confusión respectivas. de clasificacion\n",
    "## 6 comparacion de los modelos de clasificacion \n",
    "Modelos de clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, confusion_matrix, precision_score, recall_score\n",
    "import time\n",
    "\n",
    "# Predicciones con los modelos de regresión\n",
    "# Regresión con el modelo \"Same neurons as variables\"\n",
    "start = time.time()\n",
    "predictions_vars = regressorVars.predict(X_test)\n",
    "end = time.time()\n",
    "mae_vars = mean_absolute_error(y_test, predictions_vars)\n",
    "r2_vars = r2_score(y_test, predictions_vars)\n",
    "\n",
    "print(f\"Same neurons as variables: tiempo de predicción {end - start}s\")\n",
    "print(f\"Error absoluto medio: {mae_vars}\")\n",
    "print(f\"Coeficiente de determinación (R^2): {r2_vars}\\n\")\n",
    "\n",
    "# Regresión con el modelo \"Aristocratic with mini batches\"\n",
    "start = time.time()\n",
    "predictions_aristo = regressorAristo.predict(X_test)\n",
    "end = time.time()\n",
    "mae_aristo = mean_absolute_error(y_test, predictions_aristo)\n",
    "r2_aristo = r2_score(y_test, predictions_aristo)\n",
    "\n",
    "print(f\"Aristocratic with mini batches: tiempo de predicción {end - start}s\")\n",
    "print(f\"Error absoluto medio: {mae_aristo}\")\n",
    "print(f\"Coeficiente de determinación (R^2): {r2_aristo}\\n\")\n",
    "\n",
    "# Predicciones con los modelos de clasificación\n",
    "# Clasificación con el modelo \"stochastic gradient-based optimizer\"\n",
    "start = time.time()\n",
    "stcpred = stochasticClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_stc = confusion_matrix(y_test_c, stcpred)\n",
    "precision_stc = precision_score(y_test_c, stcpred, average='macro')\n",
    "recall_stc = recall_score(y_test_c, stcpred, average='macro')\n",
    "\n",
    "print(f\"stochastic gradient-based optimizer: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_stc)\n",
    "print(f\"Precisión: {precision_stc:.2f}, Recall: {recall_stc:.2f}\\n\")\n",
    "\n",
    "# Clasificación con el modelo \"gradient stochastic descent\"\n",
    "start = time.time()\n",
    "gradpred = gradienntClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_grad = confusion_matrix(y_test_c, gradpred)\n",
    "precision_grad = precision_score(y_test_c, gradpred, average='macro')\n",
    "recall_grad = recall_score(y_test_c, gradpred, average='macro')\n",
    "\n",
    "print(f\"gradient stochastic descent: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_grad)\n",
    "print(f\"Precisión: {precision_grad:.2f}, Recall: {recall_grad:.2f}\\n\")\n",
    "\n",
    "# Clasificación con el modelo \"quasi-Newton methods\"\n",
    "start = time.time()\n",
    "quasipred = quasiClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_quasi = confusion_matrix(y_test_c, quasipred)\n",
    "precision_quasi = precision_score(y_test_c, quasipred, average='macro')\n",
    "recall_quasi = recall_score(y_test_c, quasipred, average='macro')\n",
    "\n",
    "print(f\"quasi-Newton methods: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_quasi)\n",
    "print(f\"Precisión: {precision_quasi:.2f}, Recall: {recall_quasi:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import time\n",
    "\n",
    "# Clasificación con el modelo \"stochastic gradient-based optimizer\"\n",
    "start = time.time()\n",
    "stcpred = stochasticClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_stc = confusion_matrix(y_test_c, stcpred)\n",
    "precision_stc = precision_score(y_test_c, stcpred, average='macro')\n",
    "recall_stc = recall_score(y_test_c, stcpred, average='macro')\n",
    "\n",
    "print(f\"stochastic gradient-based optimizer: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_stc)\n",
    "print(f\"Precisión: {precision_stc:.2f}, Recall: {recall_stc:.2f}\\n\")\n",
    "\n",
    "# Clasificación con el modelo \"gradient stochastic descent\"\n",
    "start = time.time()\n",
    "gradpred = gradienntClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_grad = confusion_matrix(y_test_c, gradpred)\n",
    "precision_grad = precision_score(y_test_c, gradpred, average='macro')\n",
    "recall_grad = recall_score(y_test_c, gradpred, average='macro')\n",
    "\n",
    "print(f\"gradient stochastic descent: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_grad)\n",
    "print(f\"Precisión: {precision_grad:.2f}, Recall: {recall_grad:.2f}\\n\")\n",
    "\n",
    "# Clasificación con el modelo \"quasi-Newton methods\"\n",
    "start = time.time()\n",
    "quasipred = quasiClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_quasi = confusion_matrix(y_test_c, quasipred)\n",
    "precision_quasi = precision_score(y_test_c, quasipred, average='macro')\n",
    "recall_quasi = recall_score(y_test_c, quasipred, average='macro')\n",
    "\n",
    "print(f\"quasi-Newton methods: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_quasi)\n",
    "print(f\"Precisión: {precision_quasi:.2f}, Recall: {recall_quasi:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stochastic gradient-based optimizer  time 0.00410914421081543\n",
      "Matriz de Confusión:\n",
      "[[64 17  0]\n",
      " [37 91 13]\n",
      " [ 1 16 53]]\n",
      "Precisión: 0.72, Recall: 0.73\n",
      "\n",
      "gradient stochasting descent time 0.0009775161743164062\n",
      "Matriz de Confusión:\n",
      "[[  0  81   0]\n",
      " [  0 141   0]\n",
      " [  0  70   0]]\n",
      "Precisión: 0.16, Recall: 0.33\n",
      "\n",
      "quasi-Newton methods time 0.0\n",
      "Matriz de Confusión:\n",
      "[[ 7 68  6]\n",
      " [13 96 32]\n",
      " [ 1 10 59]]\n",
      "Precisión: 0.50, Recall: 0.54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score \n",
    "start = time.time()\n",
    "stcpred = stochasticClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_c, stcpred)\n",
    "precision = precision_score(y_test_c, stcpred, average='macro')\n",
    "recall = recall_score(y_test_c, stcpred, average='macro')\n",
    "\n",
    "print(f\"stochastic gradient-based optimizer  time {end - start}\")\n",
    "print(f\"Matriz de Confusión:\\n{conf_matrix}\")\n",
    "print(f\"Precisión: {precision:.2f}, Recall: {recall:.2f}\\n\")\n",
    "\n",
    "start = time.time()\n",
    "gradpred = gradienntClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"gradient stochasting descent time {end - start}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_c, gradpred)\n",
    "precision = precision_score(y_test_c, gradpred, average='macro')\n",
    "recall = recall_score(y_test_c, gradpred, average='macro')\n",
    "\n",
    "print(f\"Matriz de Confusión:\\n{conf_matrix}\")\n",
    "print(f\"Precisión: {precision:.2f}, Recall: {recall:.2f}\\n\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "quasipred = quasiClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"quasi-Newton methods time {end - start}\")\n",
    "conf_matrix = confusion_matrix(y_test_c, quasipred)\n",
    "precision = precision_score(y_test_c, quasipred, average='macro')\n",
    "recall = recall_score(y_test_c, quasipred, average='macro')\n",
    "print(f\"Matriz de Confusión:\\n{conf_matrix}\")\n",
    "print(f\"Precisión: {precision:.2f}, Recall: {recall:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Efectividad (Precisión y Recall)\n",
    "Stochastic Gradient-Based Optimizer:\n",
    "Precisión: 0.72\n",
    "Recall: 0.73\n",
    "Estos valores indican que el modelo es bastante equilibrado en términos de precisión y sensibilidad, logrando un buen rendimiento en ambas métricas.\n",
    "Gradient Stochastic Descent:\n",
    "Precisión: 0.16\n",
    "Recall: 0.33\n",
    "Este modelo tiene un rendimiento muy bajo, ya que parece no ser capaz de identificar correctamente ninguna de las categorías además de la más común, lo que resulta en una precisión y un recall muy bajos.\n",
    "Quasi-Newton Methods:\n",
    "Precisión: 0.50\n",
    "Recall: 0.54\n",
    "Este modelo muestra un rendimiento moderado, mejor que el gradient descent pero inferior al optimizer stochastic.\n",
    "2. Tiempo de Procesamiento\n",
    "Stochastic Gradient-Based Optimizer: ~0.002 s\n",
    "Gradient Stochastic Descent: ~0.001 s\n",
    "Quasi-Newton Methods: ~0.001 s\n",
    "Aunque el modelo Gradient Stochastic Descent y el Quasi-Newton Methods tienen tiempos de procesamiento ligeramente menores, la diferencia es mínima y no compensa la pérdida significativa en la precisión y el recall.\n",
    "3. Análisis de Errores\n",
    "Stochastic Gradient-Based Optimizer: El modelo parece equilibrado con errores distribuidos de manera razonable entre las clases, con una notable cantidad de aciertos en la clase media y alta.\n",
    "Gradient Stochastic Descent: Este modelo predijo todos los casos como la clase media, lo que indica un problema significativo, probablemente debido a un desequilibrio en los datos o una convergencia pobre durante el entrenamiento.\n",
    "Quasi-Newton Methods: Aunque mejor que el Gradient Stochastic Descent, este modelo aún tiene problemas, especialmente en la clase baja y alta, donde los errores son más frecuentes.\n",
    "\n",
    "\n",
    "El modelo Stochastic Gradient-Based Optimizer es claramente superior en términos de equilibrio entre precisión y recall, además de tener un tiempo de procesamiento competitivo. Los errores en este modelo tienen menos impacto en la interpretación de los resultados en comparación con los otros dos modelos, lo que lo hace más confiable para este conjunto de datos. El Gradient Stochastic Descent, en particular, muestra un rendimiento muy pobre y probablemente necesite una revisión significativa o un enfoque diferente para ser útil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analice si no hay sobreajuste en los modelos.\n",
    "## 8. Para el modelo elegido de clasificación tunee los parámetros y discuta si puede mejorar todavía el modelo sin llegar a sobre ajustarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    if axes is None:\n",
    "        axes = plt.gca()\n",
    "    axes.set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                      train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                      test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "              label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "              label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "\n",
    "models = [\n",
    "    (regressorVars, \"Learning Curve for 'Same Neurons as Variables' Model\"),\n",
    "    (regressorAristo, \"Learning Curve for 'Aristocratic with Mini Batches' Model\"),\n",
    "    (regressorHyper, \"Learning Curve for 'Hyperbolic Regression' Model\")\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "for i, (model, title) in enumerate(models):\n",
    "    plot_learning_curve(model, title, X_R, y_R, axes=axes[i], cv=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo 'Same Neurons as Variables':\n",
    "Entrenamiento vs. Validación: La curva de entrenamiento comienza con un error alto y disminuye a medida que aumenta el número de ejemplos de entrenamiento. La curva de validación también disminuye, pero se estabiliza a medida que el número de ejemplos aumenta, convergiendo con la curva de entrenamiento.\n",
    "Indicador de Sobreajuste: La convergencia de las curvas sugiere que no hay sobreajuste significativo en este modelo, ya que el modelo generaliza bien a nuevos datos.\n",
    "Modelo 'Aristocratic with Mini Batches':\n",
    "Entrenamiento vs. Validación: La curva de entrenamiento muestra un descenso consistente y se aplana, mientras que la curva de validación, aunque mejora, sigue siendo considerablemente más alta que la curva de entrenamiento.\n",
    "Indicador de Sobreajuste: Existe una brecha entre las curvas de entrenamiento y validación que indica un posible sobreajuste. El modelo podría estar aprendiendo demasiado los detalles del conjunto de entrenamiento y no generalizando bien a nuevos datos.\n",
    "Modelo 'Hyperbolic Regression':\n",
    "Entrenamiento vs. Validación: Las curvas de entrenamiento y validación están relativamente cercanas, pero hay una brecha consistente entre ellas a través del rango de tamaños de entrenamiento.\n",
    "Indicador de Sobreajuste: La brecha entre las curvas indica un leve sobreajuste. El modelo podría beneficiarse de ajustes en la regularización o en la arquitectura para mejorar la generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Preparar los datos\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_C, y_C, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el modelo y los parámetros para la búsqueda en cuadrícula\n",
    "model = MLPClassifier(random_state=1)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "# Búsqueda en cuadrícula con validación cruzada\n",
    "clf = GridSearchCV(model, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Mejores parámetros encontrados\n",
    "print(\"Mejores parámetros encontrados:\\n\", clf.best_params_)\n",
    "\n",
    "# Evaluar el modelo\n",
    "from sklearn.metrics import classification_report\n",
    "y_true, y_pred = y_test_c, clf.predict(X_test_c)\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El siguiente codigo cubre con \n",
    "### 9. Seleccione ahora el SalesPrice como variable respuesta. (declarado previamiente usando dataset anterior)\n",
    "### 10. Genere dos modelos de regresión con redes neuronales con diferentes topologías y funciones de activación para predecir el precio de las casas. (creados junto con los de clasificación)\n",
    "### 11. Compare los dos modelos de regresión y determine cuál funcionó mejor para predecir el precio de las casas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "X = df.drop('SalePrice', axis=1).values  # características\n",
    "y = df['SalePrice'].values  # variable objetivo\n",
    "\n",
    "# Dividir datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalización de características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modelo 1: Red neuronal con una capa oculta\n",
    "model1 = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', random_state=1, max_iter=500)\n",
    "start = time.time()\n",
    "model1.fit(X_train_scaled, y_train)\n",
    "end = time.time()\n",
    "predictions1 = model1.predict(X_test_scaled)\n",
    "mae1 = mean_absolute_error(y_test, predictions1)\n",
    "r2_1 = r2_score(y_test, predictions1)\n",
    "print(f\"Modelo 1 - Tiempo de entrenamiento: {end - start} segundos\")\n",
    "print(f\"Error absoluto medio (MAE): {mae1}\")\n",
    "print(f\"R^2: {r2_1}\\n\")\n",
    "\n",
    "# Modelo 2: Red neuronal con dos capas ocultas\n",
    "model2 = MLPRegressor(hidden_layer_sizes=(100, 50), activation='tanh', random_state=1, max_iter=500)\n",
    "start = time.time()\n",
    "model2.fit(X_train_scaled, y_train)\n",
    "end = time.time()\n",
    "predictions2 = model2.predict(X_test_scaled)\n",
    "mae2 = mean_absolute_error(y_test, predictions2)\n",
    "r2_2 = r2_score(y_test, predictions2)\n",
    "print(f\"Modelo 2 - Tiempo de entrenamiento: {end - start} segundos\")\n",
    "print(f\"Error absoluto medio (MAE): {mae2}\")\n",
    "print(f\"R^2: {r2_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,r2_score\n",
    "start = time.time()\n",
    "rgsVRS = regressorVars.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"Same neurons as variables  {end - start}\")\n",
    "print(f\"Error abosoluto medio {mean_absolute_error(y_test,rgsVRS)}\")\n",
    "print(f\"R^2 {r2_score(y_test,rgsVRS)}\")\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "aristocratic = regressorAristo.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"Aristocratic with mini batches time {end - start}\")\n",
    "print(f\"Error abosoluto medio {mean_absolute_error(y_test,aristocratic)}\")\n",
    "print(f\"R^2 {r2_score(y_test,aristocratic)}\")\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "hyperbolic = regressorHyper.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"hyperbolic regression model  time {end - start}\")\n",
    "print(f\"Error abosoluto medio {mean_absolute_error(y_test,hyperbolic)}\")\n",
    "print(f\"R^2 {r2_score(y_test,hyperbolic)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same neurons as variables  0.0029811859130859375\n",
      "Error abosoluto medio 29729.42215570695\n",
      "R^2 0.719712483982553\n",
      "\n",
      "Aristocratic with mini batches time 0.002945423126220703\n",
      "Error abosoluto medio 28686.403894597213\n",
      "R^2 0.7375052027117872\n",
      "\n",
      "hyperbolic regression model  time 0.002481222152709961\n",
      "Error abosoluto medio 178531.66721913655\n",
      "R^2 -4.155454018793465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error,r2_score\n",
    "start = time.time()\n",
    "rgsVRS = regressorVars.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"Same neurons as variables  {end - start}\")\n",
    "print(f\"Error abosoluto medio {mean_absolute_error(y_test,rgsVRS)}\")\n",
    "print(f\"R^2 {r2_score(y_test,rgsVRS)}\")\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "aristocratic = regressorAristo.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"Aristocratic with mini batches time {end - start}\")\n",
    "print(f\"Error abosoluto medio {mean_absolute_error(y_test,aristocratic)}\")\n",
    "print(f\"R^2 {r2_score(y_test,aristocratic)}\")\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "hyperbolic = regressorHyper.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"hyperbolic regression model  time {end - start}\")\n",
    "print(f\"Error abosoluto medio {mean_absolute_error(y_test,hyperbolic)}\")\n",
    "print(f\"R^2 {r2_score(y_test,hyperbolic)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparación:Modelo 'Same Neurons as Variables':\n",
    "MAE: 29,729.42\n",
    "R²: 0.7197\n",
    "Tiempo de Predicción: 0.0015 segundos\n",
    "Observación: Este modelo proporciona un buen equilibrio entre precisión y tiempo de ejecución, con un R² decente que indica que captura alrededor del 72% de la variabilidad en el precio de las casas.\n",
    "Modelo 'Aristocratic with Mini Batches':\n",
    "MAE: 28,686.40\n",
    "R²: 0.7375\n",
    "Tiempo de Predicción: 0.0020 segundos\n",
    "Observación: Este modelo muestra un rendimiento ligeramente mejor que el modelo 'Same Neurons as Variables' en términos de MAE y R², lo que sugiere que puede capturar una proporción mayor de la variabilidad en los datos y hacerlo con una precisión ligeramente mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Analice si no hay sobreajuste en los modelos. Use para esto la curva de aprendizaje.\n",
    "### 13. Para el modelo elegido de regresión tunee los parámetros y discuta si puede mejorar todavía el modelo sin llegar a sobre ajustarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 5), scoring='neg_mean_squared_error'):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, validation_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n",
    "    \n",
    "  \n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    validation_scores_mean = -np.mean(validation_scores, axis=1)\n",
    "    validation_scores_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, validation_scores_mean - validation_scores_std,\n",
    "                     validation_scores_mean + validation_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "plot_learning_curve(model1, \"Learning Curve for Model 1 (ReLU Activation)\", X_train_scaled, y_train, cv=5)\n",
    "plot_learning_curve(model2, \"Learning Curve for Model 2 (Tanh Activation)\", X_train_scaled, y_train, cv=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Modelo 1 muestra signos de sobreajuste como se evidencia por el aumento en el rendimiento en el conjunto de entrenamiento y el declive en el conjunto de validación después de alcanzar un pico. Aunque el modelo parece aprender efectivamente los datos de entrenamiento, su capacidad para generalizar a nuevos datos comienza a declinar a medida que se añaden más datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo 2 no hay signos claros de sobreajuste, dado que las curvas de entrenamiento y validación están cerca una de la otra y no divergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Definir el modelo base\n",
    "model = MLPRegressor(activation='tanh', random_state=1)\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50, 50), (100, 50), (100, 100)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "    'max_iter': [500, 1000]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula con validación cruzada\n",
    "grid = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mejores parámetros y resultados\n",
    "print(\"Mejores parámetros encontrados: \", grid.best_params_)\n",
    "print(\"Mejor score de validación cruzada (MSE): \", -grid.best_score_)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "best_model = grid.best_estimator_\n",
    "predictions = best_model.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"Error absoluto medio en el conjunto de prueba: \", mae)\n",
    "print(\"R^2 en el conjunto de prueba: \", r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
