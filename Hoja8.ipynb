{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja 8 minería de datos\n",
    "## Pasos 1 a 2\n",
    "\n",
    "### PASO 2\n",
    "sale price será variable de respuesta, mendiante cuartiles se divide en barato, medio y cara. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      OverallQual  YearBuilt  YearRemodAdd  TotalBsmtSF  1stFlrSF  GrLivArea  \\\n",
      "0               7       2003          2003          856       856       1710   \n",
      "1               6       1976          1976         1262      1262       1262   \n",
      "2               7       2001          2002          920       920       1786   \n",
      "3               7       1915          1970          756       961       1717   \n",
      "4               8       2000          2000         1145      1145       2198   \n",
      "...           ...        ...           ...          ...       ...        ...   \n",
      "1455            6       1999          2000          953       953       1647   \n",
      "1456            6       1978          1988         1542      2073       2073   \n",
      "1457            7       1941          2006         1152      1188       2340   \n",
      "1458            5       1950          1996         1078      1078       1078   \n",
      "1459            5       1965          1965         1256      1256       1256   \n",
      "\n",
      "      FullBath  TotRmsAbvGrd  GarageCars  GarageArea  SalePrice PriceCategory  \n",
      "0            2             8           2         548     208500             1  \n",
      "1            2             6           2         460     181500             1  \n",
      "2            2             6           2         608     223500             2  \n",
      "3            1             7           3         642     140000             1  \n",
      "4            2             9           3         836     250000             2  \n",
      "...        ...           ...         ...         ...        ...           ...  \n",
      "1455         2             7           2         460     175000             1  \n",
      "1456         2             7           2         500     210000             1  \n",
      "1457         2             9           1         252     266500             2  \n",
      "1458         1             5           1         240     142125             1  \n",
      "1459         1             6           1         276     147500             1  \n",
      "\n",
      "[1460 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dataset de hojas anteriores ( inciso 1)\n",
    "\n",
    "df  = pd.read_csv(\"train.csv\")\n",
    "numeric_columns = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n",
    "                   'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "                   'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n",
    "                   'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
    "                   'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n",
    "                   'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
    "                   'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold','SalePrice']\n",
    "numeric_df = df[numeric_columns]\n",
    "corr_matrix = numeric_df.corr()\n",
    "saleprice_correlation = corr_matrix['SalePrice'].sort_values(ascending=False)\n",
    "# Filtrar las variables numéricas con correlación menor a 0.5 con SalePrice\n",
    "to_drop = saleprice_correlation[saleprice_correlation < 0.5].index.tolist()\n",
    "cleaned_df = numeric_df.drop(columns=to_drop)\n",
    "\n",
    "p25 = cleaned_df['SalePrice'].quantile(0.25)\n",
    "p75 = cleaned_df['SalePrice'].quantile(0.75)\n",
    "cleaned_df['PriceCategory'] = pd.cut(cleaned_df['SalePrice'], bins=[0, p25, p75, float('inf')],\n",
    "                               labels=[0,1,2], right=False)\n",
    "df = cleaned_df\n",
    "del cleaned_df,numeric_columns,to_drop,corr_matrix,saleprice_correlation,p25,p75\n",
    "print(df)\n",
    "# variable de respuesta ['SalePrice'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Rregression AI\n",
    "y_R = df['SalePrice']\n",
    "X_R = df.drop('SalePrice',axis=1).drop('PriceCategory',axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R, y_R, test_size=0.2, random_state=42)\n",
    "\n",
    "#Same neurons as variables (uses lineal regression)\n",
    "regressorVars = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
    "#Aristocratic with mini batches (uses lineal regression)\n",
    "regressorAristo = MLPRegressor(random_state=1, max_iter=500,batch_size=min(200, X_R.shape[1])).fit(X_train, y_train)\n",
    "#hyperbolic regression model \n",
    "regressorHyper = MLPRegressor(random_state=1,max_iter=500,activation='tanh').fit(X_train, y_train)\n",
    "\n",
    "#Classification AI\n",
    "y_C = df['PriceCategory']\n",
    "X_C = df.drop('SalePrice',axis=1).drop('PriceCategory',axis=1)\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_C, y_C, test_size=0.2, random_state=42)\n",
    "\n",
    "#default classifier\n",
    "stochasticClassifier = MLPClassifier(random_state=1, max_iter=300, solver='adam').fit(X_train_c, y_train_c) #stochastic gradient-based optimizer \n",
    "gradienntClassifier = MLPClassifier(random_state=1, max_iter=300, solver='sgd').fit(X_train_c, y_train_c) #gradient stochasting descent\n",
    "quasiClassifier = MLPClassifier(random_state=1, max_iter=300, solver='lbfgs').fit(X_train_c, y_train_c) #quasi-Newton methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use los modelos para predecir el valor de la variable respuesta\n",
    "## 5. Haga las matrices de confusión respectivas. de clasificacion\n",
    "## 6 comparacion de los modelos de clasificacion \n",
    "Modelos de clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, confusion_matrix, precision_score, recall_score\n",
    "import time\n",
    "\n",
    "# Predicciones con los modelos de regresión\n",
    "# Regresión con el modelo \"Same neurons as variables\"\n",
    "start = time.time()\n",
    "predictions_vars = regressorVars.predict(X_test)\n",
    "end = time.time()\n",
    "mae_vars = mean_absolute_error(y_test, predictions_vars)\n",
    "r2_vars = r2_score(y_test, predictions_vars)\n",
    "\n",
    "print(f\"Same neurons as variables: tiempo de predicción {end - start}s\")\n",
    "print(f\"Error absoluto medio: {mae_vars}\")\n",
    "print(f\"Coeficiente de determinación (R^2): {r2_vars}\\n\")\n",
    "\n",
    "# Regresión con el modelo \"Aristocratic with mini batches\"\n",
    "start = time.time()\n",
    "predictions_aristo = regressorAristo.predict(X_test)\n",
    "end = time.time()\n",
    "mae_aristo = mean_absolute_error(y_test, predictions_aristo)\n",
    "r2_aristo = r2_score(y_test, predictions_aristo)\n",
    "\n",
    "print(f\"Aristocratic with mini batches: tiempo de predicción {end - start}s\")\n",
    "print(f\"Error absoluto medio: {mae_aristo}\")\n",
    "print(f\"Coeficiente de determinación (R^2): {r2_aristo}\\n\")\n",
    "\n",
    "# Predicciones con los modelos de clasificación\n",
    "# Clasificación con el modelo \"stochastic gradient-based optimizer\"\n",
    "start = time.time()\n",
    "stcpred = stochasticClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_stc = confusion_matrix(y_test_c, stcpred)\n",
    "precision_stc = precision_score(y_test_c, stcpred, average='macro')\n",
    "recall_stc = recall_score(y_test_c, stcpred, average='macro')\n",
    "\n",
    "print(f\"stochastic gradient-based optimizer: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_stc)\n",
    "print(f\"Precisión: {precision_stc:.2f}, Recall: {recall_stc:.2f}\\n\")\n",
    "\n",
    "# Clasificación con el modelo \"gradient stochastic descent\"\n",
    "start = time.time()\n",
    "gradpred = gradienntClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_grad = confusion_matrix(y_test_c, gradpred)\n",
    "precision_grad = precision_score(y_test_c, gradpred, average='macro')\n",
    "recall_grad = recall_score(y_test_c, gradpred, average='macro')\n",
    "\n",
    "print(f\"gradient stochastic descent: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_grad)\n",
    "print(f\"Precisión: {precision_grad:.2f}, Recall: {recall_grad:.2f}\\n\")\n",
    "\n",
    "# Clasificación con el modelo \"quasi-Newton methods\"\n",
    "start = time.time()\n",
    "quasipred = quasiClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_quasi = confusion_matrix(y_test_c, quasipred)\n",
    "precision_quasi = precision_score(y_test_c, quasipred, average='macro')\n",
    "recall_quasi = recall_score(y_test_c, quasipred, average='macro')\n",
    "\n",
    "print(f\"quasi-Newton methods: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_quasi)\n",
    "print(f\"Precisión: {precision_quasi:.2f}, Recall: {recall_quasi:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import time\n",
    "\n",
    "# Clasificación con el modelo \"stochastic gradient-based optimizer\"\n",
    "start = time.time()\n",
    "stcpred = stochasticClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_stc = confusion_matrix(y_test_c, stcpred)\n",
    "precision_stc = precision_score(y_test_c, stcpred, average='macro')\n",
    "recall_stc = recall_score(y_test_c, stcpred, average='macro')\n",
    "\n",
    "print(f\"stochastic gradient-based optimizer: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_stc)\n",
    "print(f\"Precisión: {precision_stc:.2f}, Recall: {recall_stc:.2f}\\n\")\n",
    "\n",
    "# Clasificación con el modelo \"gradient stochastic descent\"\n",
    "start = time.time()\n",
    "gradpred = gradienntClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_grad = confusion_matrix(y_test_c, gradpred)\n",
    "precision_grad = precision_score(y_test_c, gradpred, average='macro')\n",
    "recall_grad = recall_score(y_test_c, gradpred, average='macro')\n",
    "\n",
    "print(f\"gradient stochastic descent: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_grad)\n",
    "print(f\"Precisión: {precision_grad:.2f}, Recall: {recall_grad:.2f}\\n\")\n",
    "\n",
    "# Clasificación con el modelo \"quasi-Newton methods\"\n",
    "start = time.time()\n",
    "quasipred = quasiClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "conf_matrix_quasi = confusion_matrix(y_test_c, quasipred)\n",
    "precision_quasi = precision_score(y_test_c, quasipred, average='macro')\n",
    "recall_quasi = recall_score(y_test_c, quasipred, average='macro')\n",
    "\n",
    "print(f\"quasi-Newton methods: tiempo de predicción {end - start}s\")\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix_quasi)\n",
    "print(f\"Precisión: {precision_quasi:.2f}, Recall: {recall_quasi:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stochastic gradient-based optimizer  time 0.00410914421081543\n",
      "Matriz de Confusión:\n",
      "[[64 17  0]\n",
      " [37 91 13]\n",
      " [ 1 16 53]]\n",
      "Precisión: 0.72, Recall: 0.73\n",
      "\n",
      "gradient stochasting descent time 0.0009775161743164062\n",
      "Matriz de Confusión:\n",
      "[[  0  81   0]\n",
      " [  0 141   0]\n",
      " [  0  70   0]]\n",
      "Precisión: 0.16, Recall: 0.33\n",
      "\n",
      "quasi-Newton methods time 0.0\n",
      "Matriz de Confusión:\n",
      "[[ 7 68  6]\n",
      " [13 96 32]\n",
      " [ 1 10 59]]\n",
      "Precisión: 0.50, Recall: 0.54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score \n",
    "start = time.time()\n",
    "stcpred = stochasticClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_c, stcpred)\n",
    "precision = precision_score(y_test_c, stcpred, average='macro')\n",
    "recall = recall_score(y_test_c, stcpred, average='macro')\n",
    "\n",
    "print(f\"stochastic gradient-based optimizer  time {end - start}\")\n",
    "print(f\"Matriz de Confusión:\\n{conf_matrix}\")\n",
    "print(f\"Precisión: {precision:.2f}, Recall: {recall:.2f}\\n\")\n",
    "\n",
    "start = time.time()\n",
    "gradpred = gradienntClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"gradient stochasting descent time {end - start}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_c, gradpred)\n",
    "precision = precision_score(y_test_c, gradpred, average='macro')\n",
    "recall = recall_score(y_test_c, gradpred, average='macro')\n",
    "\n",
    "print(f\"Matriz de Confusión:\\n{conf_matrix}\")\n",
    "print(f\"Precisión: {precision:.2f}, Recall: {recall:.2f}\\n\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "quasipred = quasiClassifier.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"quasi-Newton methods time {end - start}\")\n",
    "conf_matrix = confusion_matrix(y_test_c, quasipred)\n",
    "precision = precision_score(y_test_c, quasipred, average='macro')\n",
    "recall = recall_score(y_test_c, quasipred, average='macro')\n",
    "print(f\"Matriz de Confusión:\\n{conf_matrix}\")\n",
    "print(f\"Precisión: {precision:.2f}, Recall: {recall:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Efectividad (Precisión y Recall)\n",
    "Stochastic Gradient-Based Optimizer:\n",
    "Precisión: 0.72\n",
    "Recall: 0.73\n",
    "Estos valores indican que el modelo es bastante equilibrado en términos de precisión y sensibilidad, logrando un buen rendimiento en ambas métricas.\n",
    "Gradient Stochastic Descent:\n",
    "Precisión: 0.16\n",
    "Recall: 0.33\n",
    "Este modelo tiene un rendimiento muy bajo, ya que parece no ser capaz de identificar correctamente ninguna de las categorías además de la más común, lo que resulta en una precisión y un recall muy bajos.\n",
    "Quasi-Newton Methods:\n",
    "Precisión: 0.50\n",
    "Recall: 0.54\n",
    "Este modelo muestra un rendimiento moderado, mejor que el gradient descent pero inferior al optimizer stochastic.\n",
    "2. Tiempo de Procesamiento\n",
    "Stochastic Gradient-Based Optimizer: ~0.002 s\n",
    "Gradient Stochastic Descent: ~0.001 s\n",
    "Quasi-Newton Methods: ~0.001 s\n",
    "Aunque el modelo Gradient Stochastic Descent y el Quasi-Newton Methods tienen tiempos de procesamiento ligeramente menores, la diferencia es mínima y no compensa la pérdida significativa en la precisión y el recall.\n",
    "3. Análisis de Errores\n",
    "Stochastic Gradient-Based Optimizer: El modelo parece equilibrado con errores distribuidos de manera razonable entre las clases, con una notable cantidad de aciertos en la clase media y alta.\n",
    "Gradient Stochastic Descent: Este modelo predijo todos los casos como la clase media, lo que indica un problema significativo, probablemente debido a un desequilibrio en los datos o una convergencia pobre durante el entrenamiento.\n",
    "Quasi-Newton Methods: Aunque mejor que el Gradient Stochastic Descent, este modelo aún tiene problemas, especialmente en la clase baja y alta, donde los errores son más frecuentes.\n",
    "\n",
    "\n",
    "El modelo Stochastic Gradient-Based Optimizer es claramente superior en términos de equilibrio entre precisión y recall, además de tener un tiempo de procesamiento competitivo. Los errores en este modelo tienen menos impacto en la interpretación de los resultados en comparación con los otros dos modelos, lo que lo hace más confiable para este conjunto de datos. El Gradient Stochastic Descent, en particular, muestra un rendimiento muy pobre y probablemente necesite una revisión significativa o un enfoque diferente para ser útil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analice si no hay sobreajuste en los modelos.\n",
    "## 8. Para el modelo elegido de clasificación tunee los parámetros y discuta si puede mejorar todavía el modelo sin llegar a sobre ajustarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El siguiente codigo cubre con \n",
    "### 9. Seleccione ahora el SalesPrice como variable respuesta. (declarado previamiente usando dataset anterior)\n",
    "### 10. Genere dos modelos de regresión con redes neuronales con diferentes topologías y funciones de activación para predecir el precio de las casas. (creados junto con los de clasificación)\n",
    "### 11. Compare los dos modelos de regresión y determine cuál funcionó mejor para predecir el precio de las casas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same neurons as variables  0.0029811859130859375\n",
      "Error abosoluto medio 29729.42215570695\n",
      "R^2 0.719712483982553\n",
      "\n",
      "Aristocratic with mini batches time 0.002945423126220703\n",
      "Error abosoluto medio 28686.403894597213\n",
      "R^2 0.7375052027117872\n",
      "\n",
      "hyperbolic regression model  time 0.002481222152709961\n",
      "Error abosoluto medio 178531.66721913655\n",
      "R^2 -4.155454018793465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error,r2_score\n",
    "start = time.time()\n",
    "rgsVRS = regressorVars.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"Same neurons as variables  {end - start}\")\n",
    "print(f\"Error abosoluto medio {mean_absolute_error(y_test,rgsVRS)}\")\n",
    "print(f\"R^2 {r2_score(y_test,rgsVRS)}\")\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "aristocratic = regressorAristo.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"Aristocratic with mini batches time {end - start}\")\n",
    "print(f\"Error abosoluto medio {mean_absolute_error(y_test,aristocratic)}\")\n",
    "print(f\"R^2 {r2_score(y_test,aristocratic)}\")\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "hyperbolic = regressorHyper.predict(X_test_c)\n",
    "end = time.time()\n",
    "print(f\"hyperbolic regression model  time {end - start}\")\n",
    "print(f\"Error abosoluto medio {mean_absolute_error(y_test,hyperbolic)}\")\n",
    "print(f\"R^2 {r2_score(y_test,hyperbolic)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Analice si no hay sobreajuste en los modelos. Use para esto la curva de aprendizaje.\n",
    "### 13. Para el modelo elegido de regresión tunee los parámetros y discuta si puede mejorar todavía el modelo sin llegar a sobre ajustarlo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
